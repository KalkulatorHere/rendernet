{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:05:14.528219Z","iopub.execute_input":"2025-04-05T19:05:14.528465Z","iopub.status.idle":"2025-04-05T19:05:15.457713Z","shell.execute_reply.started":"2025-04-05T19:05:14.528445Z","shell.execute_reply":"2025-04-05T19:05:15.456815Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    pipeline\n)\nimport numpy as np\nfrom sklearn.metrics import accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:05:15.458770Z","iopub.execute_input":"2025-04-05T19:05:15.459236Z","iopub.status.idle":"2025-04-05T19:05:39.015457Z","shell.execute_reply.started":"2025-04-05T19:05:15.459211Z","shell.execute_reply":"2025-04-05T19:05:39.014360Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# --------------------------\n# 1. Data Loading & Cleaning\n# --------------------------\n# Load raw data\ndf = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")  # Update path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:05:39.016459Z","iopub.execute_input":"2025-04-05T19:05:39.017082Z","iopub.status.idle":"2025-04-05T19:05:40.363000Z","shell.execute_reply.started":"2025-04-05T19:05:39.017054Z","shell.execute_reply":"2025-04-05T19:05:40.362290Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Function to use only a fraction of data\ndef subset_data(dataframe, fraction=1.0, random_state=42):\n    \"\"\"\n    Returns a subset of the dataframe\n    \n    Args:\n        dataframe: Pandas DataFrame to subset\n        fraction: Fraction of data to use (0-1)\n        random_state: Random seed for reproducibility\n        \n    Returns:\n        Subset of the original dataframe\n    \"\"\"\n    return dataframe.sample(frac=fraction, random_state=random_state)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:05:40.363763Z","iopub.execute_input":"2025-04-05T19:05:40.363981Z","iopub.status.idle":"2025-04-05T19:05:40.368089Z","shell.execute_reply.started":"2025-04-05T19:05:40.363962Z","shell.execute_reply":"2025-04-05T19:05:40.367154Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Apply subset if needed (adjust fraction as needed)\nUSE_SUBSET = True  # Set to False to use all data\nSUBSET_FRACTION = 1.0  # Use 10% of data (adjust as needed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:05:49.792740Z","iopub.execute_input":"2025-04-05T19:05:49.793033Z","iopub.status.idle":"2025-04-05T19:05:49.796545Z","shell.execute_reply.started":"2025-04-05T19:05:49.793013Z","shell.execute_reply":"2025-04-05T19:05:49.795696Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"if USE_SUBSET:\n    print(f\"Using {SUBSET_FRACTION*100}% of the original dataset ({int(len(df)*SUBSET_FRACTION)} samples)\")\n    df = subset_data(df, fraction=SUBSET_FRACTION)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:05:50.228124Z","iopub.execute_input":"2025-04-05T19:05:50.228371Z","iopub.status.idle":"2025-04-05T19:05:50.239437Z","shell.execute_reply.started":"2025-04-05T19:05:50.228350Z","shell.execute_reply":"2025-04-05T19:05:50.238690Z"}},"outputs":[{"name":"stdout","text":"Using 100.0% of the original dataset (50000 samples)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Clean NSFW/violent content using GPU batch processing\ntoxicity_pipe = pipeline(\n    \"text-classification\",\n    model=\"unitary/toxic-bert\",\n    device=0,\n    truncation=True,\n    max_length=512,\n    top_k=None,\n    batch_size=512\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:05:50.883923Z","iopub.execute_input":"2025-04-05T19:05:50.884191Z","iopub.status.idle":"2025-04-05T19:05:54.631914Z","shell.execute_reply.started":"2025-04-05T19:05:50.884171Z","shell.execute_reply":"2025-04-05T19:05:54.630989Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14a57d2588e443c4abdae917ea9379c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"442af110664a47bfbed61b7042d37ca2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/174 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1db6510b72a34bd8999fe76ad0e01336"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"309713f2b89849108a3c8eb8792903f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47ebc12b6da74ab881b67b1e068f2c2b"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def filter_toxic(batch):\n    \"\"\"Filter out toxic reviews with batch processing\"\"\"\n    results = toxicity_pipe(batch[\"review\"])\n    clean_indices = []\n    for idx, result in enumerate(results):\n        toxic = any(entry[\"score\"] > 0.5 for entry in result if entry[\"label\"] in [\"toxic\", \"obscene\", \"threat\"])\n        if not toxic:\n            clean_indices.append(idx)\n    return {\"review\": [batch[\"review\"][i] for i in clean_indices],\n            \"sentiment\": [batch[\"sentiment\"][i] for i in clean_indices]}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:05:54.633095Z","iopub.execute_input":"2025-04-05T19:05:54.633411Z","iopub.status.idle":"2025-04-05T19:05:54.638650Z","shell.execute_reply.started":"2025-04-05T19:05:54.633378Z","shell.execute_reply":"2025-04-05T19:05:54.637779Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Convert to HuggingFace Dataset and filter\ndataset = Dataset.from_pandas(df)\ncleaned_dataset = dataset.filter(filter_toxic, batched=True, batch_size=256)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:05:54.640007Z","iopub.execute_input":"2025-04-05T19:05:54.640213Z","iopub.status.idle":"2025-04-05T19:33:06.738105Z","shell.execute_reply.started":"2025-04-05T19:05:54.640194Z","shell.execute_reply":"2025-04-05T19:33:06.737383Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5da64899cfd4d8fb955db876640f1aa"}},"metadata":{}},{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# --------------------------\n# 2. Dataset Preparation\n# --------------------------\n# Split dataset\ntrain_test = cleaned_dataset.train_test_split(test_size=0.1)\ndataset = DatasetDict({\n    \"train\": train_test[\"train\"],\n    \"test\": train_test[\"test\"]\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:33:06.739154Z","iopub.execute_input":"2025-04-05T19:33:06.739382Z","iopub.status.idle":"2025-04-05T19:33:06.751514Z","shell.execute_reply.started":"2025-04-05T19:33:06.739361Z","shell.execute_reply":"2025-04-05T19:33:06.750815Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Print dataset statistics\nprint(f\"Training samples: {len(dataset['train'])}\")\nprint(f\"Testing samples: {len(dataset['test'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:33:06.752409Z","iopub.execute_input":"2025-04-05T19:33:06.752671Z","iopub.status.idle":"2025-04-05T19:33:06.758547Z","shell.execute_reply.started":"2025-04-05T19:33:06.752650Z","shell.execute_reply":"2025-04-05T19:33:06.757906Z"}},"outputs":[{"name":"stdout","text":"Training samples: 352\nTesting samples: 40\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Tokenization\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\ndef tokenize(batch):\n    return tokenizer(\n        batch[\"review\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=512\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:33:39.141561Z","iopub.execute_input":"2025-04-05T19:33:39.141881Z","iopub.status.idle":"2025-04-05T19:33:40.188995Z","shell.execute_reply.started":"2025-04-05T19:33:39.141854Z","shell.execute_reply":"2025-04-05T19:33:40.188311Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7eb1f96e0294e63bc56fdb574543e31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4aae29f9009f46c0a921ad70dea988a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76591079860249e589fb30767ec74183"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50305ae7a1a04a0fac4d3dc5238dc2ee"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Reduce batch size for mapping if using smaller dataset\nmapping_batch_size = 128 if USE_SUBSET else 256\ndataset = dataset.map(tokenize, batched=True, batch_size=mapping_batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:33:43.810237Z","iopub.execute_input":"2025-04-05T19:33:43.810541Z","iopub.status.idle":"2025-04-05T19:33:44.246926Z","shell.execute_reply.started":"2025-04-05T19:33:43.810515Z","shell.execute_reply":"2025-04-05T19:33:44.246144Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/352 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08fa42a834b14056b5be2c7fca200785"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44c1a2e6ae984b2aaa534b76f6343161"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Convert labels to 0/1\ndef format_labels(batch):\n    return {\"labels\": [1 if s == \"positive\" else 0 for s in batch[\"sentiment\"]]}\ndataset = dataset.map(format_labels, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:33:52.561538Z","iopub.execute_input":"2025-04-05T19:33:52.561883Z","iopub.status.idle":"2025-04-05T19:33:52.606487Z","shell.execute_reply.started":"2025-04-05T19:33:52.561856Z","shell.execute_reply":"2025-04-05T19:33:52.605547Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/352 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bf20da652104a6fbc28a283bbed7b8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d8536cf43f3450dad2caa08506620c9"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# Set PyTorch format\ndataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:33:57.003801Z","iopub.execute_input":"2025-04-05T19:33:57.004105Z","iopub.status.idle":"2025-04-05T19:33:57.008756Z","shell.execute_reply.started":"2025-04-05T19:33:57.004082Z","shell.execute_reply":"2025-04-05T19:33:57.007895Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# --------------------------\n# 3. Model Training (Clean Data)\n# --------------------------\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\",\n    num_labels=2\n).to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:33:57.226772Z","iopub.execute_input":"2025-04-05T19:33:57.227016Z","iopub.status.idle":"2025-04-05T19:33:58.782520Z","shell.execute_reply.started":"2025-04-05T19:33:57.226995Z","shell.execute_reply":"2025-04-05T19:33:58.781644Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39c25efebbbc4eea9962ccffdf4c4da3"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Adjust batch size based on dataset size\ntrain_batch_size = 64 if USE_SUBSET else 256\neval_batch_size = 32 if USE_SUBSET else 128","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:33:58.783630Z","iopub.execute_input":"2025-04-05T19:33:58.783900Z","iopub.status.idle":"2025-04-05T19:33:58.787658Z","shell.execute_reply.started":"2025-04-05T19:33:58.783878Z","shell.execute_reply":"2025-04-05T19:33:58.786664Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Training arguments with progress tracking\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"steps\",\n    logging_steps=50 if USE_SUBSET else 100,  # More frequent logging for small dataset\n    per_device_train_batch_size=train_batch_size,\n    per_device_eval_batch_size=eval_batch_size,\n    num_train_epochs=30,\n    learning_rate=2e-5,\n    warmup_steps=100 if USE_SUBSET else 500,  # Fewer warmup steps for smaller dataset\n    weight_decay=0.01,\n    report_to=\"none\",\n    disable_tqdm=False,  # Force show progress\n    fp16=True,  # Use mixed precision\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:42:35.569364Z","iopub.execute_input":"2025-04-05T19:42:35.569736Z","iopub.status.idle":"2025-04-05T19:42:35.604274Z","shell.execute_reply.started":"2025-04-05T19:42:35.569665Z","shell.execute_reply":"2025-04-05T19:42:35.603295Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# Metrics for validation\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    return {\"accuracy\": accuracy_score(p.label_ids, preds)}\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    compute_metrics=compute_metrics\n)\n\n# Start training with visible progress\nprint(\"\\n\\n=== Starting Training ===\")\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:42:37.699410Z","iopub.execute_input":"2025-04-05T19:42:37.699746Z","iopub.status.idle":"2025-04-05T19:47:06.936082Z","shell.execute_reply.started":"2025-04-05T19:42:37.699706Z","shell.execute_reply":"2025-04-05T19:47:06.935370Z"}},"outputs":[{"name":"stdout","text":"\n\n=== Starting Training ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [90/90 04:25, Epoch 30/30]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.303723</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>1.306026</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>1.309144</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>1.314605</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>1.321690</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>1.329551</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>1.338239</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>1.409124</td>\n      <td>0.850000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>1.355285</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>No log</td>\n      <td>1.370621</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>No log</td>\n      <td>1.378301</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>No log</td>\n      <td>1.390006</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>No log</td>\n      <td>1.403723</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>No log</td>\n      <td>1.418791</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>No log</td>\n      <td>1.439665</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>No log</td>\n      <td>1.451643</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.000000</td>\n      <td>1.468844</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.000000</td>\n      <td>1.550860</td>\n      <td>0.850000</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.000000</td>\n      <td>1.508968</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.000000</td>\n      <td>1.787797</td>\n      <td>0.825000</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.000000</td>\n      <td>1.550194</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.000000</td>\n      <td>1.612478</td>\n      <td>0.825000</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.000000</td>\n      <td>1.660998</td>\n      <td>0.825000</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.000000</td>\n      <td>1.620658</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.000000</td>\n      <td>1.941637</td>\n      <td>0.825000</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.000000</td>\n      <td>1.674305</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.000000</td>\n      <td>2.040380</td>\n      <td>0.825000</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.000000</td>\n      <td>1.734552</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.000000</td>\n      <td>2.140339</td>\n      <td>0.825000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.000000</td>\n      <td>1.798524</td>\n      <td>0.875000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=90, training_loss=1.6549977105266103e-05, metrics={'train_runtime': 268.6928, 'train_samples_per_second': 39.301, 'train_steps_per_second': 0.335, 'total_flos': 1398855729807360.0, 'train_loss': 1.6549977105266103e-05, 'epoch': 30.0})"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# --------------------------\n# 4. Noisy Data Experiment\n# --------------------------\n# Flip 10% of training labels\nnp.random.seed(42)\ntrain_df = dataset[\"train\"].to_pandas()\nflip_indices = np.random.choice(\n    train_df.index,\n    size=int(0.1 * len(train_df)),\n    replace=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:47:06.937356Z","iopub.execute_input":"2025-04-05T19:47:06.937714Z","iopub.status.idle":"2025-04-05T19:47:06.946732Z","shell.execute_reply.started":"2025-04-05T19:47:06.937688Z","shell.execute_reply":"2025-04-05T19:47:06.946061Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"train_df.loc[flip_indices, \"labels\"] = 1 - train_df.loc[flip_indices, \"labels\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:47:06.948111Z","iopub.execute_input":"2025-04-05T19:47:06.948319Z","iopub.status.idle":"2025-04-05T19:47:06.966797Z","shell.execute_reply.started":"2025-04-05T19:47:06.948300Z","shell.execute_reply":"2025-04-05T19:47:06.966107Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Convert back to Dataset\nnoisy_train = Dataset.from_pandas(train_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:47:06.967932Z","iopub.execute_input":"2025-04-05T19:47:06.968375Z","iopub.status.idle":"2025-04-05T19:47:06.996550Z","shell.execute_reply.started":"2025-04-05T19:47:06.968334Z","shell.execute_reply":"2025-04-05T19:47:06.995703Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Initialize a fresh model for noisy training (avoids using partially trained model)\nnoisy_model = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\",\n    num_labels=2\n).to(\"cuda\")\nnoisy_training_args = TrainingArguments(\n    output_dir=\"./results_noisy\",\n    per_device_train_batch_size=train_batch_size // 2,  # Smaller batch for noisy data\n    per_device_eval_batch_size=eval_batch_size,\n    num_train_epochs=40,\n    learning_rate=1e-5,  # Lower LR for fine-tuning on noisy data\n    fp16=True,\n    logging_steps=50 if USE_SUBSET else 100,\n    logging_strategy=\"steps\",\n    evaluation_strategy=\"epoch\",  # Add evaluation every epoch\n    disable_tqdm=False,\n    report_to = 'none')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:47:06.997507Z","iopub.execute_input":"2025-04-05T19:47:06.997841Z","iopub.status.idle":"2025-04-05T19:47:07.289539Z","shell.execute_reply.started":"2025-04-05T19:47:06.997810Z","shell.execute_reply":"2025-04-05T19:47:07.288780Z"}},"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"    # Removed report_to parameter that was causing the error\n\n# Retrain with noisy data\nnoisy_trainer = Trainer(\n    model=noisy_model,  # Use fresh model\n    args=noisy_training_args,\n    train_dataset=noisy_train,\n    eval_dataset=dataset[\"test\"],  # Still evaluate on clean test data\n    compute_metrics=compute_metrics,\n    \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:47:07.290275Z","iopub.execute_input":"2025-04-05T19:47:07.290473Z","iopub.status.idle":"2025-04-05T19:47:07.301615Z","shell.execute_reply.started":"2025-04-05T19:47:07.290455Z","shell.execute_reply":"2025-04-05T19:47:07.300992Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"print(\"\\n\\n=== Training with Noisy Data ===\")\nnoisy_trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:47:07.302310Z","iopub.execute_input":"2025-04-05T19:47:07.302489Z","iopub.status.idle":"2025-04-05T19:53:29.797051Z","shell.execute_reply.started":"2025-04-05T19:47:07.302471Z","shell.execute_reply":"2025-04-05T19:53:29.796225Z"}},"outputs":[{"name":"stdout","text":"\n\n=== Training with Noisy Data ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [240/240 06:20, Epoch 40/40]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.686585</td>\n      <td>0.525000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.660801</td>\n      <td>0.650000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.636088</td>\n      <td>0.725000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.602443</td>\n      <td>0.825000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.548049</td>\n      <td>0.825000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.485279</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>0.429935</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>0.376994</td>\n      <td>0.925000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.589900</td>\n      <td>0.333679</td>\n      <td>0.925000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.589900</td>\n      <td>0.305963</td>\n      <td>0.925000</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.589900</td>\n      <td>0.290753</td>\n      <td>0.925000</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.589900</td>\n      <td>0.298771</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.589900</td>\n      <td>0.275133</td>\n      <td>0.925000</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.589900</td>\n      <td>0.275478</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.589900</td>\n      <td>0.272684</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.589900</td>\n      <td>0.278379</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.278800</td>\n      <td>0.283797</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.278800</td>\n      <td>0.292223</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.278800</td>\n      <td>0.300552</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.278800</td>\n      <td>0.297970</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.278800</td>\n      <td>0.300561</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.278800</td>\n      <td>0.304865</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.278800</td>\n      <td>0.304343</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.278800</td>\n      <td>0.324004</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.141300</td>\n      <td>0.333136</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.141300</td>\n      <td>0.334987</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.141300</td>\n      <td>0.361139</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.141300</td>\n      <td>0.358049</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.141300</td>\n      <td>0.353783</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.141300</td>\n      <td>0.365170</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.141300</td>\n      <td>0.371243</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.141300</td>\n      <td>0.367677</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.141300</td>\n      <td>0.363563</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.087100</td>\n      <td>0.374527</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.087100</td>\n      <td>0.383102</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.087100</td>\n      <td>0.387814</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.087100</td>\n      <td>0.391985</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.087100</td>\n      <td>0.392539</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.087100</td>\n      <td>0.392584</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.087100</td>\n      <td>0.393271</td>\n      <td>0.875000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=240, training_loss=0.23904554843902587, metrics={'train_runtime': 381.9432, 'train_samples_per_second': 36.864, 'train_steps_per_second': 0.628, 'total_flos': 1865140973076480.0, 'train_loss': 0.23904554843902587, 'epoch': 40.0})"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}